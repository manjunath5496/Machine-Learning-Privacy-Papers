# Machine Learning Privacy Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(1).pdf" style="text-decoration:none;">Black-box Model Inversion Attribute Inference Attacks on Classification Models</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(2).pdf" style="text-decoration:none;">An Attack-Based Evaluation Method for Differentially Private Learning Against Model Inversion Attack</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(3).pdf" style="text-decoration:none;">Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(4).pdf" style="text-decoration:none;">Exploiting Explanations for Model Inversion Attacks</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(5).pdf" style="text-decoration:none;">GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(6).pdf" style="text-decoration:none;">Exploring Connections Between Active Learning and Model Extraction</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(7).pdf" style="text-decoration:none;">Rethinking Privacy Preserving Deep Learning: How to Evaluate and Thwart Privacy Attacks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(8).pdf" style="text-decoration:none;"> Simulating Unknown Target Models for Query-Efficient Black-box Attacks </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(9).pdf" style="text-decoration:none;">Inverting Gradients - How easy is it to break privacy in federated learning?</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(10).pdf" style="text-decoration:none;">Model Extraction and Defenses on Generative Adversarial Networks </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(11).pdf" style="text-decoration:none;">Membership Inference Attack Susceptibility of Clinical Language Models</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(12).pdf" style="text-decoration:none;">Deep Leakage from Gradients</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(13).pdf" style="text-decoration:none;">Protecting Decision Boundary of Machine Learning Model With Differentially Private Perturbation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(14).pdf" style="text-decoration:none;">CSI NN: Reverse Engineering of Neural Network Architectures Through Electromagnetic Side Channel</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(15).pdf" style="text-decoration:none;">LOGAN: Membership Inference Attacks Against Generative Models</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(16).pdf" style="text-decoration:none;">Revisiting Membership Inference Under Realistic Assumptions</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(17).pdf" style="text-decoration:none;">Systematic Evaluation of Privacy Risks of Machine Learning Models</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(18).pdf" style="text-decoration:none;">Unexpected Information Leakage of Differential Privacy Due to Linear Property of Queries</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(19).pdf" style="text-decoration:none;">Improving Robustness to Model Inversion Attacks via Mutual Information Regularization</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(20).pdf" style="text-decoration:none;">Differentially Private Learning Does Not Bound Membership Inference</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(21).pdf" style="text-decoration:none;">Privacy Risks of Securing Machine Learning Models against Adversarial Examples</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(22).pdf" style="text-decoration:none;">Deep Neural Network Fingerprinting by Conferrable Adversarial Examples</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(23).pdf" style="text-decoration:none;">ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(24).pdf" style="text-decoration:none;">Privacy and Security Issues in Deep Learning: A Survey</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(25).pdf" style="text-decoration:none;">SAPAG: A Self-Adaptive Privacy Attack From Gradients</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(26).pdf" style="text-decoration:none;">Knockoff Nets: Stealing Functionality of Black-Box Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(27).pdf" style="text-decoration:none;">Privacy Risks of General-Purpose Language Models</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(28).pdf" style="text-decoration:none;">Extracting Training Data from Large Language Models</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(29).pdf" style="text-decoration:none;">Membership Leakage in Label-Only Exposures </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(30).pdf" style="text-decoration:none;">An Overview of Privacy in Machine Learning</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(31).pdf" style="text-decoration:none;">Reconstruction-Based Membership Inference Attacks are Easier on Difficult Problems</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(32).pdf" style="text-decoration:none;">Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Generative Adversarial Networks</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(33).pdf" style="text-decoration:none;">Theory-Oriented Deep Leakage from Gradients via Linear Equation Solver</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(34).pdf" style="text-decoration:none;">MixCon: Adjusting the Separability of Data Representations for Harder Data Recovery</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(35).pdf" style="text-decoration:none;">Honest-but-Curious Nets: Sensitive Attributes of Private Inputs can be Secretly Coded into the Entropy of Classifiers' Outputs</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(36).pdf" style="text-decoration:none;">R-GAP: Recursive Gradient Attack on Privacy</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(37).pdf" style="text-decoration:none;">Black-Box Ripper: Copying black-box models using generative evolutionary algorithms</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(38).pdf" style="text-decoration:none;">I Know What You See: Power Side-Channel Attack on Convolutional Neural Network Accelerators</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(39).pdf" style="text-decoration:none;">A Survey of Privacy Attacks in Machine Learning</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(40).pdf" style="text-decoration:none;">Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(41).pdf" style="text-decoration:none;">BODAME: Bilevel Optimization for Defense Against Model Extraction</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(42).pdf" style="text-decoration:none;">KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(43).pdf" style="text-decoration:none;">Towards Reverse-Engineering Black-Box Neural Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(44).pdf" style="text-decoration:none;">PrivGAN: Protecting GANs from membership inference attacks at low cost</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(45).pdf" style="text-decoration:none;">DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(46).pdf" style="text-decoration:none;">Membership Inference Attacks on Machine Learning: A Survey</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(47).pdf" style="text-decoration:none;">White-box vs Black-box: Bayes Optimal Strategies for Membership Inference</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(48).pdf" style="text-decoration:none;">Auditing Data Provenance in Text-Generation Models</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(49).pdf" style="text-decoration:none;">Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(50).pdf" style="text-decoration:none;">Efficiently Stealing your Machine Learning Models</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(51).pdf" style="text-decoration:none;">Alleviating Privacy Attacks via Causal Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(52).pdf" style="text-decoration:none;">Evaluation of Inference Attack Models for Deep Learning on Medical Data</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(53).pdf" style="text-decoration:none;">Stealing Neural Network Models through the Scan Chain: A New Threat for ML Hardware</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(54).pdf" style="text-decoration:none;">Privacy Analysis of Deep Learning in the Wild: Membership Inference Attacks against Transfer Learning </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(55).pdf" style="text-decoration:none;">Robust Membership Encoding: Inference Attacks and Copyright Protection for Deep Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(56).pdf" style="text-decoration:none;">Understanding Membership Inferences on Well-Generalized Learning Models </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(57).pdf" style="text-decoration:none;">Hermes Attack: Steal DNN Models with Lossless Inference Accuracy</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(58).pdf" style="text-decoration:none;">Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(59).pdf" style="text-decoration:none;">Membership Inference Attacks on Deep Regression Models for Neuroimaging</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(60).pdf" style="text-decoration:none;">IReEn: Iterative Reverse-Engineering of Black-Box Functions via Neural Program Synthesis </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(61).pdf" style="text-decoration:none;">Use the Spear as a Shield: A Novel Adversarial Example based Privacy-Preserving Technique against Membership Inference Attacks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(62).pdf" style="text-decoration:none;">MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(63).pdf" style="text-decoration:none;">Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(64).pdf" style="text-decoration:none;">GAMIN: An Adversarial Approach to Black-Box Model Inversion</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(65).pdf" style="text-decoration:none;">Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System? </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(66).pdf" style="text-decoration:none;">Property Inference Attacks on Convolutional Neural Networks: Influence and Implications of Target Model's Complexity</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(67).pdf" style="text-decoration:none;">Illuminating the Dark or how to recover what should not be seen in FE-based classiers</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(68).pdf" style="text-decoration:none;">FaceLeaks: Inference Attacks against Transfer Learning Models via Black-box Queries</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(69).pdf" style="text-decoration:none;">Bootstrap Aggregation for Point-based Generalized Membership Inference Attacks</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(70).pdf" style="text-decoration:none;">Disparate Vulnerability: on the Unfairness of Privacy Attacks Against Machine Learning</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(71).pdf" style="text-decoration:none;">MACE: A Framework for Membership Privacy Estimation in Generative Models</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(72).pdf" style="text-decoration:none;">CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(73).pdf" style="text-decoration:none;">Evaluation Indicator for Model Inversion Attack</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(74).pdf" style="text-decoration:none;">Membership Inference Attacks Against Machine Learning Models</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(75).pdf" style="text-decoration:none;">The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(76).pdf" style="text-decoration:none;">When Machine Unlearning Jeopardizes Privacy</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(77).pdf" style="text-decoration:none;">Stealing Hyperparameters in Machine Learning</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(78).pdf" style="text-decoration:none;">Against Membership Inference Attack: Pruning is All You Need</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(79).pdf" style="text-decoration:none;">TransMIA: Membership Inference Attacks Using Transfer Shadow Training</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(80).pdf" style="text-decoration:none;">Membership Inference Attack against Differentially Private Deep Learning Model</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(81).pdf" style="text-decoration:none;">On the Privacy Risks of Model Explanations</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(82).pdf" style="text-decoration:none;">Node-Level Membership Inference Attacks Against Graph Neural Networks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(83).pdf" style="text-decoration:none;">Model extraction from counterfactual explanations</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(84).pdf" style="text-decoration:none;">Security Analysis of Deep Neural Networks Operating in the Presence of Cache Side-Channel Attacks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(85).pdf" style="text-decoration:none;">Practical Blind Membership Inference Attack via Differential Comparisons</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(86).pdf" style="text-decoration:none;">Protecting DNNs from Theft using an Ensemble of Diverse Models</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(87).pdf" style="text-decoration:none;">The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(88).pdf" style="text-decoration:none;">Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(89).pdf" style="text-decoration:none;">Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning</a></li>
  
  
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(90).pdf" style="text-decoration:none;"> Property Inference from Poisoning</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(91).pdf" style="text-decoration:none;">Label-Only Membership Inference Attacks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(92).pdf" style="text-decoration:none;">Cache Telepathy: Leveraging Shared Resource Attacks to Learn DNN Architectures</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(93).pdf" style="text-decoration:none;"> Quantifying Membership Privacy via Information Leakage</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(94).pdf" style="text-decoration:none;">Evaluating Differentially Private Machine Learning in Practice</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(95).pdf" style="text-decoration:none;">ML-DOCTOR: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models</a></li>  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(96).pdf" style="text-decoration:none;">Information Laundering for Model Privacy</a></li> 
  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(97).pdf" style="text-decoration:none;">Dataset Inference: Ownership Resolution in Machine Learning</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(98).pdf" style="text-decoration:none;">Exploiting Unintended Feature Leakage in Collaborative Learning</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(99).pdf" style="text-decoration:none;">ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models</a></li>  
  
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(100).pdf" style="text-decoration:none;">Cryptanalytic Extraction of
Neural Network Models</a></li>  
  
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(101).pdf" style="text-decoration:none;">Improved Techniques for Model Inversion Attacks</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(102).pdf" style="text-decoration:none;">Leveraging Extracted Model Adversaries for Improved Black Box Attacks</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(103).pdf" style="text-decoration:none;">Sampling Attacks: Amplification of Membership Inference Attacks by Repeated Queries </a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(104).pdf" style="text-decoration:none;">Segmentations-Leak: Membership Inference Attacks and Defenses in Semantic Image Segmentation</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(105).pdf" style="text-decoration:none;">iDLG: Improved Deep Leakage from Gradients</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(106).pdf" style="text-decoration:none;">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(107).pdf" style="text-decoration:none;">Sharing Models or Coresets: A Study based on Membership Inference Attack</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(108).pdf" style="text-decoration:none;">ACTIVETHIEF: Model Extraction Using Active Learning and Unannotated Public Data</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(109).pdf" style="text-decoration:none;">ModelWeight TheftWith Just Noise Inputs: The Curious Case of the Petulant Attacker</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(110).pdf" style="text-decoration:none;">Practical Defences Against Model Inversion Attacks for Split Neural Networks </a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(111).pdf" style="text-decoration:none;">Reconstruction of training samples from loss functions</a></li> 
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(112).pdf" style="text-decoration:none;">High Accuracy and High Fidelity Extraction of Neural Networks</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(113).pdf" style="text-decoration:none;">Stealing Machine Learning Models via Prediction APIs</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(114).pdf" style="text-decoration:none;">Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(115).pdf" style="text-decoration:none;">On the Difficulty of Membership Inference Attacks</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(116).pdf" style="text-decoration:none;">Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(117).pdf" style="text-decoration:none;">The Influence of Dropout on Membership Inference in Differentially Private Models</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(118).pdf" style="text-decoration:none;">How to Own NAS in Your Spare Time</a></li>  
   
  <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(119).pdf" style="text-decoration:none;">Differential Privacy Defenses and Sampling Attacks for Membership Inference</a></li> 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(120).pdf" style="text-decoration:none;">Trade-offs and Guarantees of Adversarial Representation Learning for Information Obfuscation</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(121).pdf" style="text-decoration:none;">Thief, Beware of What Get You There: Towards Understanding Model Extraction Attack</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(122).pdf" style="text-decoration:none;">A Methodology for Formalizing Model-Inversion Attacks </a></li>  
     
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(123).pdf" style="text-decoration:none;">On the Effectiveness of Regularization Against Membership Inference Attacks</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(124).pdf" style="text-decoration:none;">An Extension of Fanoâ€™s Inequality for Characterizing Model Susceptibility to Membership Inference Attacks</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(125).pdf" style="text-decoration:none;">Extraction of Complex DNN Models: Real Threat or Boogeyman?</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(126).pdf" style="text-decoration:none;">Membership Inference Attacks on Knowledge Graphs</a></li> 
   
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(127).pdf" style="text-decoration:none;">Does AI Remember?
Neural Networks and the Right to be Forgotten</a></li>  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(128).pdf" style="text-decoration:none;">Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(129).pdf" style="text-decoration:none;">Quantifying Membership Inference Vulnerability via Generalization Gap and Other Model Metrics</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(130).pdf" style="text-decoration:none;">ADePT: Auto-encoder based Differentially Private Text Transformation</a></li>    
   
<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(131).pdf" style="text-decoration:none;">Overlearning Reveals Sensitive Attributes</a></li>   
   
   <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(132).pdf" style="text-decoration:none;">Stealing Neural Networks via Timing Side Channels</a></li>   
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(133).pdf" style="text-decoration:none;">Comprehensive Privacy Analysis of Deep Learning</a></li>     
   
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(134).pdf" style="text-decoration:none;">Modelling and Quantifying Membership Information Leakage in Machine Learning</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(135).pdf" style="text-decoration:none;">Reducing Risk of Model Inversion Using Privacy-Guided Training</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Machine-Learning-Privacy-Papers/blob/master/m(136).pdf" style="text-decoration:none;">On Primes, Log-Loss Scores and (No) Privacy</a></li>
 </ul>
   
   
   
